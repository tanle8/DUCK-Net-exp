{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "857a5f70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T23:30:26.687048Z",
     "start_time": "2024-07-14T23:30:26.674320Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "import tensorflow as tf\n",
    "import albumentations as albu\n",
    "import numpy as np\n",
    "import gc\n",
    "from keras.callbacks import CSVLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "from model_architecture.DiceLoss import dice_metric_loss\n",
    "from model_architecture import DUCK_Net\n",
    "from image_loader import ImageLoader2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7852dbbf-f3a4-4db6-8c5e-98889bf3abd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T23:26:37.712049Z",
     "start_time": "2024-07-14T23:26:37.699985Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Checking the number of GPUs available\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNum GPUs Available: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlist_physical_devices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Checking the number of GPUs available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"TanLD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9edbe667",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T23:26:59.302867Z",
     "start_time": "2024-07-14T23:26:59.283911Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m seed_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m58800\u001b[39m\n\u001b[1;32m      9\u001b[0m filters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m17\u001b[39m \u001b[38;5;66;03m# Number of filters, the paper presents the results with 17 and 34\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mRMSprop(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Current timestamp for file naming\u001b[39;00m\n\u001b[1;32m     13\u001b[0m ct \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Setting the model parameters\n",
    "img_size = 352\n",
    "model_type = \"DuckNet\"\n",
    "dataset_type = 'kvasir' # Options: kvasir/cvc-clinicdb/cvc-colondb/etis-laribpolypdb\n",
    "learning_rate = 1e-4\n",
    "EPOCHS = 600\n",
    "min_loss_for_saving = 0.2\n",
    "seed_value = 58800\n",
    "filters = 17 # Number of filters, the paper presents the results with 17 and 34\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "# Current timestamp for file naming\n",
    "ct = datetime.now()\n",
    "\n",
    "# Paths setup using pathlib for cross-platform compatibility\n",
    "# Setting base_path to the current working directory\n",
    "base_path = Path.cwd()\n",
    "\n",
    "# Setup for directories based on the current working directory\n",
    "logs_path = base_path / \"logs\"\n",
    "models_path = base_path / \"models\"\n",
    "results_path = base_path / \"results\"\n",
    "\n",
    "# Ensure the directories exist\n",
    "logs_path.mkdir(parents=True, exist_ok=True)\n",
    "models_path.mkdir(parents=True, exist_ok=True)\n",
    "results_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "progress_csv_path = logs_path / f\"{model_type}_filters_{filters}_{ct}.csv\"\n",
    "progress_txt_path = logs_path / f\"{model_type}_filters_{filters}_{ct}.txt\"\n",
    "plot_path = results_path / f\"{model_type}_filters_{filters}_{ct}.png\"\n",
    "model_save_path = models_path / f\"{model_type}_filters_{filters}_{ct}.h5\"\n",
    "\n",
    "# Ensure the paths are converted to strings when used with non-Pathlib compatible functions\n",
    "csv_logger = CSVLogger(str(progress_csv_path), append=True, separator=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ddcf82296a6fca",
   "metadata": {},
   "source": [
    "## EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae47ecf202f44ed0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T23:27:56.696842Z",
     "start_time": "2024-07-14T23:27:56.690532Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1133109215.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    if img is not mode None:\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder, filename))\n",
    "        if img is not mode None:\n",
    "            images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75f36e0ca9d391a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T23:29:48.021602Z",
     "start_time": "2024-07-14T23:29:48.000669Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_images_from_folder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m kvasir_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/Kvasir-SEG/train/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mload_images_from_folder\u001b[49m(kvasir_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m masks \u001b[38;5;241m=\u001b[39m load_images_from_folder(kvasir_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_images_from_folder' is not defined"
     ]
    }
   ],
   "source": [
    "kvasir_path = 'datasets/Kvasir-SEG/train/'\n",
    "\n",
    "images = load_images_from_folder(kvasir_path + 'images')\n",
    "masks = load_images_from_folder(kvasir_path + 'masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad9888b0e1a57d28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T23:30:13.217376Z",
     "start_time": "2024-07-14T23:30:13.140845Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m         axs[i, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 12\u001b[0m show_images(\u001b[43mimages\u001b[49m, masks)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "def show_images(images, masks, num=5):\n",
    "    fig, axs = plt.subplots(num, 2, figsize=(10, num * 3))\n",
    "    for i in range(num):\n",
    "        axs[i, 0].imshow(images[i])\n",
    "        axs[i, 0].set_title('Image')\n",
    "        axs[i, 1].imshow(masks[i], cmap='gray')\n",
    "        axs[i, 1].set_title('Mask')\n",
    "        axs[i, 0].axis('off')\n",
    "        axs[i, 1].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_images(images, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b52f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "wandb.init(project=\"polyp-segmentation\", entity=\"tanld\", config={\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"img_size\": img_size,\n",
    "    \"model_type\": model_type,\n",
    "    \"dataset_type\": dataset_type,\n",
    "    \"filters\": filters,\n",
    "    \"optimizer\": \"RMSprop\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264050d8",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "X, Y = ImageLoader2D.load_data(img_size, img_size, -1, 'kvasir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75b6c029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data, seed for reproducibility\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, shuffle= True, random_state = seed_value)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.111, shuffle= True, random_state = seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92a7b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the augmentations\n",
    "\n",
    "aug_train = albu.Compose([\n",
    "    albu.HorizontalFlip(),\n",
    "    albu.VerticalFlip(),\n",
    "    albu.ColorJitter(brightness=(0.6,1.6), contrast=0.2, saturation=0.1, hue=0.01, always_apply=True),\n",
    "    albu.Affine(scale=(0.5,1.5), translate_percent=(-0.125,0.125), rotate=(-180,180), shear=(-22.5,22), always_apply=True),\n",
    "])\n",
    "\n",
    "def augment_images():\n",
    "    x_train_out = []\n",
    "    y_train_out = []\n",
    "\n",
    "    for i in range (len(x_train)):\n",
    "        ug = aug_train(image=x_train[i], mask=y_train[i])\n",
    "        x_train_out.append(ug['image'])  \n",
    "        y_train_out.append(ug['mask'])\n",
    "\n",
    "    return np.array(x_train_out), np.array(y_train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1609dd32",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "\n",
    "model = DUCK_Net.create_model(img_height=img_size, img_width=img_size, input_chanels=3, out_classes=1, starting_filters=filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e513d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=dice_metric_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef712b9",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "step = 0\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    \n",
    "    print(f'Training, epoch {epoch}')\n",
    "    print('Learning Rate: ' + str(learning_rate))\n",
    "\n",
    "    step += 1\n",
    "        \n",
    "    image_augmented, mask_augmented = augment_images()\n",
    "    \n",
    "    csv_logger = CSVLogger(progress_path, append=True, separator=';')\n",
    "    \n",
    "    # Run one epoch of training\n",
    "    resutl = model.fit(x=image_augmented, y=mask_augmented, epochs=1, batch_size=4, validation_data=(x_valid, y_valid), verbose=1, callbacks=[csv_logger])\n",
    "\n",
    "    # Logging metrics to wandb\n",
    "    wandb.log({\n",
    "        \"train_loss\": result.history['loss'][0],\n",
    "        \"val_loss\": result.history['val_loss'][0],\n",
    "        \"train_accuracy\": result.history['accuracy'][0],\n",
    "        \"val_accuracy\": result.history['val_accuracy'][0],\n",
    "        \"epoch\": epoch\n",
    "    })\n",
    "    \n",
    "    prediction_valid = model.predict(x_valid, verbose=0)\n",
    "    loss_valid = dice_metric_loss(y_valid, prediction_valid).numpy()\n",
    "\n",
    "    print(\"Loss Validation: \" + str(loss_valid))\n",
    "        \n",
    "    prediction_test = model.predict(x_test, verbose=0)\n",
    "    loss_test = dice_metric_loss(y_test, prediction_test)\n",
    "    loss_test = loss_test.numpy()\n",
    "    print(\"Loss Test: \" + str(loss_test))\n",
    "        \n",
    "    with open(progressfull_path, 'a') as f:\n",
    "        f.write('epoch: ' + str(epoch) + '\\nval_loss: ' + str(loss_valid) + '\\ntest_loss: ' + str(loss_test) + '\\n\\n\\n')\n",
    "    \n",
    "    if min_loss_for_saving > loss_valid:\n",
    "        min_loss_for_saving = loss_valid\n",
    "        print(\"Saved model with val_loss: \", loss_valid)\n",
    "        model.save(model_save_path)\n",
    "        wandb.run.summary[\"best_val_loss\"] = loss_valid\n",
    "        wandb.save(model_save_path)\n",
    "        \n",
    "    del image_augmented\n",
    "    del mask_augmented\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd52447",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Computing the metrics and saving the results\n",
    "\n",
    "print(\"Loading the model\")\n",
    "\n",
    "model = tf.keras.models.load_model(model_save_path, custom_objects={'dice_metric_loss':dice_metric_loss})\n",
    "\n",
    "prediction_train = model.predict(x_train, batch_size=4)\n",
    "prediction_valid = model.predict(x_valid, batch_size=4)\n",
    "prediction_test = model.predict(x_test, batch_size=4)\n",
    "\n",
    "print(\"Predictions done\")\n",
    "\n",
    "dice_train = f1_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_train > 0.5))\n",
    "dice_test = f1_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                          np.ndarray.flatten(prediction_test > 0.5))\n",
    "dice_valid = f1_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Dice finished\")\n",
    "\n",
    "\n",
    "miou_train = jaccard_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_train > 0.5))\n",
    "miou_test = jaccard_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                          np.ndarray.flatten(prediction_test > 0.5))\n",
    "miou_valid = jaccard_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Miou finished\")\n",
    "\n",
    "\n",
    "precision_train = precision_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                                  np.ndarray.flatten(prediction_train > 0.5))\n",
    "precision_test = precision_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                                 np.ndarray.flatten(prediction_test > 0.5))\n",
    "precision_valid = precision_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                                  np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Precision finished\")\n",
    "\n",
    "\n",
    "recall_train = recall_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                            np.ndarray.flatten(prediction_train > 0.5))\n",
    "recall_test = recall_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_test > 0.5))\n",
    "recall_valid = recall_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                            np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Recall finished\")\n",
    "\n",
    "\n",
    "accuracy_train = accuracy_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                                np.ndarray.flatten(prediction_train > 0.5))\n",
    "accuracy_test = accuracy_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                               np.ndarray.flatten(prediction_test > 0.5))\n",
    "accuracy_valid = accuracy_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                                np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "\n",
    "print(\"Accuracy finished\")\n",
    "\n",
    "\n",
    "final_file = 'results_' + model_type + '_' + str(filters) + '_' + dataset_type + '.txt'\n",
    "print(final_file)\n",
    "\n",
    "with open(final_file, 'a') as f:\n",
    "    f.write(dataset_type + '\\n\\n')\n",
    "    f.write('dice_train: ' + str(dice_train) + ' dice_valid: ' + str(dice_valid) + ' dice_test: ' + str(dice_test) + '\\n\\n')\n",
    "    f.write('miou_train: ' + str(miou_train) + ' miou_valid: ' + str(miou_valid) + ' miou_test: ' + str(miou_test) + '\\n\\n')\n",
    "    f.write('precision_train: ' + str(precision_train) + ' precision_valid: ' + str(precision_valid) + ' precision_test: ' + str(precision_test) + '\\n\\n')\n",
    "    f.write('recall_train: ' + str(recall_train) + ' recall_valid: ' + str(recall_valid) + ' recall_test: ' + str(recall_test) + '\\n\\n')\n",
    "    f.write('accuracy_train: ' + str(accuracy_train) + ' accuracy_valid: ' + str(accuracy_valid) + ' accuracy_test: ' + str(accuracy_test) + '\\n\\n\\n\\n')\n",
    "\n",
    "print('File done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the wandb run to clean up resources and finalize the logging session\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
